Loading policy parameters from /snapshots/iter0000000 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000000
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 112.258693672
Loading policy parameters from /snapshots/iter0000020 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000020
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 252.42580916
Loading policy parameters from /snapshots/iter0000040 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000040
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 332.032909356
Fitting the GMM with 5 components on the expert trajectories at ../trajectories/expert_trajectories-Humanoid
Dataset size: 237128 transitions (250 trajectories)
Average return: 9525.75615224

Beginning the analysis...
======================================================

Generating 100 trajectories for policy iter0000000...
Calculating scores...
Dataset size: 2390 transitions (100 trajectories)
Average return: 112.258693672
<type 'numpy.ndarray'>
Dataset size: 2390 transitions (100 trajectories)
Average return: 112.258693672
<type 'numpy.ndarray'>
upper_bound_reward:  168.769428564
Mean score of good states: -177242210.799922, bad states: 429.890317
Std score of good states: 3569122070.403206, bad states: 1025.599369
Min score of good states: -90418932946.249496, bad states: -25157.889916
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -25157.889916
10 percentile: 120.896609
20 percentile: 353.926398
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -90418932946.249496
10 percentile: -410.586356
20 percentile: -86.336348
30 percentile: 90.551340
40 percentile: 218.989963
50 percentile: 307.718939
60 percentile: 363.228848
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000020...
Calculating scores...
Dataset size: 5152 transitions (100 trajectories)
Average return: 252.42580916
<type 'numpy.ndarray'>
Dataset size: 5152 transitions (100 trajectories)
Average return: 252.42580916
<type 'numpy.ndarray'>
upper_bound_reward:  335.688130055
Mean score of good states: -246308.732817, bad states: -4635.255335
Std score of good states: 6401532.232310, bad states: 108173.769206
Min score of good states: -182422340.347769, bad states: -2845602.929102
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -2845602.929102
10 percentile: 140.920778
20 percentile: 301.357603
30 percentile: 419.923321
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -182422340.347769
10 percentile: -486.443294
20 percentile: -181.858185
30 percentile: -2.253932
40 percentile: 81.533048
50 percentile: 155.971311
60 percentile: 233.429424
70 percentile: 300.716179
80 percentile: 353.604048
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000040...
Calculating scores...
Dataset size: 6574 transitions (100 trajectories)
Average return: 332.032909356
<type 'numpy.ndarray'>
Dataset size: 6574 transitions (100 trajectories)
Average return: 332.032909356
<type 'numpy.ndarray'>
upper_bound_reward:  420.45863126
Mean score of good states: -29878599.313819, bad states: -10234839.006297
Std score of good states: 653331385.190334, bad states: 210740265.890865
Min score of good states: -19294818346.047710, bad states: -5184916138.968135
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -5184916138.968135
10 percentile: 251.073163
20 percentile: 332.875613
30 percentile: 394.380675
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -19294818346.047710
10 percentile: -239.824840
20 percentile: 121.578713
30 percentile: 264.981333
40 percentile: 309.740150
50 percentile: 344.689781
60 percentile: 371.499549
70 percentile: 394.933838
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 1Loading policy parameters from /snapshots/iter0000060 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000060
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 329.688936357
Loading policy parameters from /snapshots/iter0000080 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000080
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 324.939005889
Loading policy parameters from /snapshots/iter0000100 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000100
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 341.206838379
Loading policy parameters from /snapshots/iter0000120 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000120
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 405.189702169
00 trajectories for policy iter0000060...
Calculating scores...
Dataset size: 6337 transitions (100 trajectories)
Average return: 329.688936357
<type 'numpy.ndarray'>
Dataset size: 6337 transitions (100 trajectories)
Average return: 329.688936357
<type 'numpy.ndarray'>
upper_bound_reward:  389.048451786
Mean score of good states: -10886833.426241, bad states: -54457072.073625
Std score of good states: 315345062.008576, bad states: 1731631244.816674
Min score of good states: -10022905107.661928, bad states: -55330922993.177727
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -55330922993.177727
10 percentile: 204.897311
20 percentile: 342.838548
30 percentile: 380.089086
40 percentile: 405.166465
50 percentile: 518.371790
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -10022905107.661928
10 percentile: -101.148607
20 percentile: 166.902618
30 percentile: 335.453215
40 percentile: 365.955507
50 percentile: 383.841871
60 percentile: 398.991040
70 percentile: 417.789346
80 percentile: 443.769806
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000080...
Calculating scores...
Dataset size: 6104 transitions (100 trajectories)
Average return: 324.939005889
<type 'numpy.ndarray'>
Dataset size: 6104 transitions (100 trajectories)
Average return: 324.939005889
<type 'numpy.ndarray'>
upper_bound_reward:  368.191943217
Mean score of good states: -42850510.228590, bad states: -53425.818494
Std score of good states: 1204254689.117959, bad states: 971626.922641
Min score of good states: -37450220826.158745, bad states: -21608024.790868
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -21608024.790868
10 percentile: 243.492937
20 percentile: 363.991326
30 percentile: 391.020223
40 percentile: 411.806873
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -37450220826.158745
10 percentile: -86.387020
20 percentile: 230.807535
30 percentile: 298.994193
40 percentile: 359.209977
50 percentile: 388.938799
60 percentile: 402.754197
70 percentile: 416.910865
80 percentile: 435.139086
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000100...
Calculating scores...
Dataset size: 6351 transitions (100 trajectories)
Average return: 341.206838379
<type 'numpy.ndarray'>
Dataset size: 6351 transitions (100 trajectories)
Average return: 341.206838379
<type 'numpy.ndarray'>
upper_bound_reward:  439.107534832
Mean score of good states: -10988183.853001, bad states: -51941.459141
Std score of good states: 350291827.338808, bad states: 1228235.181851
Min score of good states: -11301431235.717762, bad states: -37843624.221863
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -37843624.221863
10 percentile: 215.295480
20 percentile: 382.423954
30 percentile: 406.058318
40 percentile: 422.260408
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -11301431235.717762
10 percentile: -163.013250
20 percentile: 133.548668
30 percentile: 257.019713
40 percentile: 329.524279
50 percentile: 374.887683
60 percentile: 397.406307
70 percentile: 416.877603
80 percentile: 430.641750
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000120...
Calculating scores...
Dataset size: 7342 transitions (100 trajectories)
Average return: 405.189702169
<type 'numpy.ndarray'>
Dataset size: 7342 transitions (100 trajectories)
Average return: 405.189702169
<type 'numLoading policy parameters from /snapshots/iter0000140 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000140
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 446.216687587
Loading policy parameters from /snapshots/iter0000160 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000160
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 505.266334076
Loading policy parameters from /snapshots/iter0000180 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000180
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 569.217931354
py.ndarray'>
upper_bound_reward:  542.783253278
Mean score of good states: -22297735.798919, bad states: 447.733419
Std score of good states: 368910897.634501, bad states: 810.051187
Min score of good states: -10727007000.528450, bad states: -20674.375102
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -20674.375102
10 percentile: 352.706665
20 percentile: 408.137151
30 percentile: 431.465336
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -10727007000.528450
10 percentile: -217.718683
20 percentile: 114.785561
30 percentile: 258.871797
40 percentile: 353.493292
50 percentile: 393.389100
60 percentile: 418.232176
70 percentile: 436.373318
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000140...
Calculating scores...
Dataset size: 7827 transitions (100 trajectories)
Average return: 446.216687587
<type 'numpy.ndarray'>
Dataset size: 7827 transitions (100 trajectories)
Average return: 446.216687587
<type 'numpy.ndarray'>
upper_bound_reward:  613.036300732
Mean score of good states: -16384119.112440, bad states: 490.863847
Std score of good states: 470122413.591455, bad states: 135.015496
Min score of good states: -16973711231.958361, bad states: -344.973273
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -344.973273
10 percentile: 356.966631
20 percentile: 404.656294
30 percentile: 433.432703
40 percentile: 452.867789
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -16973711231.958361
10 percentile: -153.670928
20 percentile: 216.159502
30 percentile: 298.163919
40 percentile: 352.529063
50 percentile: 390.881144
60 percentile: 409.905647
70 percentile: 424.686015
80 percentile: 440.835804
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000160...
Calculating scores...
Dataset size: 8557 transitions (100 trajectories)
Average return: 505.266334076
<type 'numpy.ndarray'>
Dataset size: 8557 transitions (100 trajectories)
Average return: 505.266334076
<type 'numpy.ndarray'>
upper_bound_reward:  686.046265685
Mean score of good states: -229709.308964, bad states: -3705.549633
Std score of good states: 7236232.351054, bad states: 158365.809900
Min score of good states: -268552632.863001, bad states: -6011028.176798
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -6011028.176798
10 percentile: 367.383525
20 percentile: 415.252023
30 percentile: 434.239639
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -268552632.863001
10 percentile: 36.801870
20 percentile: 276.964357
30 percentile: 341.098015
40 percentile: 385.399395
50 percentile: 406.048690
60 percentile: 422.686602
70 percentile: 438.395637
80 percentile: 457.989283
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000180...
Calculating scores...
Dataset size: 9361 transitions (100 trajectories)
Average return: 569.217931354
<type 'numpy.ndarray'>
Dataset size: 9361 transitions (100 trajectories)
Average return: 569.217931354
<type 'numpy.ndarray'>
upper_bound_reward:  769.231710367
Mean score of good states: -47987923.760556, bad states: 480.668092
Std score of good states: 1375237243.614434, bad states: 317.018048
Min score of good states: -46144712964.205513, bad states: -10246.829642
Max score of good states: 583.422610, bad states: 583.4Loading policy parameters from /snapshots/iter0000200 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000200
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 604.929837912
Loading policy parameters from /snapshots/iter0000220 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000220
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 702.138091508
Loading policy parameters from /snapshots/iter0000240 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000240
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 728.875100087
22610
Bad percentiles:
0 percentile: -10246.829642
10 percentile: 288.875196
20 percentile: 409.073774
30 percentile: 434.376126
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -46144712964.205513
10 percentile: 115.818813
20 percentile: 264.616562
30 percentile: 336.192567
40 percentile: 380.041704
50 percentile: 402.440535
60 percentile: 420.608391
70 percentile: 437.548829
80 percentile: 455.557209
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000200...
Calculating scores...
Dataset size: 9705 transitions (100 trajectories)
Average return: 604.929837912
<type 'numpy.ndarray'>
Dataset size: 9705 transitions (100 trajectories)
Average return: 604.929837912
<type 'numpy.ndarray'>
upper_bound_reward:  807.203525154
Mean score of good states: 196.955763, bad states: -5655.564194
Std score of good states: 2210.232500, bad states: 168893.327555
Min score of good states: -67702.684324, bad states: -5426073.267987
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -5426073.267987
10 percentile: 308.776710
20 percentile: 403.456461
30 percentile: 431.838415
40 percentile: 453.066144
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -67702.684324
10 percentile: 203.127404
20 percentile: 316.750657
30 percentile: 368.526848
40 percentile: 396.916563
50 percentile: 416.138285
60 percentile: 431.576529
70 percentile: 445.021845
80 percentile: 466.605846
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000220...
Calculating scores...
Dataset size: 11005 transitions (100 trajectories)
Average return: 702.138091508
<type 'numpy.ndarray'>
Dataset size: 11005 transitions (100 trajectories)
Average return: 702.138091508
<type 'numpy.ndarray'>
upper_bound_reward:  1007.96534722
Mean score of good states: -17131126.506528, bad states: 512.945111
Std score of good states: 580041513.321572, bad states: 134.296036
Min score of good states: -25249653373.642681, bad states: -636.151389
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -636.151389
10 percentile: 385.351469
20 percentile: 426.558920
30 percentile: 455.416830
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -25249653373.642681
10 percentile: 166.059298
20 percentile: 338.814684
30 percentile: 381.467647
40 percentile: 409.309705
50 percentile: 429.386939
60 percentile: 445.809299
70 percentile: 469.450032
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000240...
Calculating scores...
Dataset size: 11250 transitions (100 trajectories)
Average return: 728.875100087
<type 'numpy.ndarray'>
Dataset size: 11250 transitions (100 trajectories)
Average return: 728.875100087
<type 'numpy.ndarray'>
upper_bound_reward:  993.028730034
Mean score of good states: -4485354.731634, bad states: 511.860453
Std score of good states: 153374442.449010, bad states: 127.930264
Min score of good states: -6687382837.647371, bad states: -486.564503
Max score of good states: 583.422610, bad states: 583.422610
Bad percentiles:
0 percentile: -486.564503
10 percentile: 364.626771
20 percentile: 427.970525
30 percentile: 456.263181
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
Loading policy parameters from /snapshots/iter0000260 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000260
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 818.60922981
Loading policy parameters from /snapshots/iter0000280 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000280
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 835.653447817
Loading policy parameters from /snapshots/iter0000300 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000300
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 975.87854421
0 percentile: -6687382837.647371
10 percentile: 252.627491
20 percentile: 351.489395
30 percentile: 393.495836
40 percentile: 414.851348
50 percentile: 431.192473
60 percentile: 445.307910
70 percentile: 468.097705
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610

======================================================

Generating 100 trajectories for policy iter0000260...
Calculating scores...
Dataset size: 12269 transitions (100 trajectories)
Average return: 818.60922981
<type 'numpy.ndarray'>
Dataset size: 12269 transitions (100 trajectories)
Average return: 818.60922981
<type 'numpy.ndarray'>
upper_bound_reward:  1091.72363615
Mean score of good states: 357.380133, bad states: 461.483727
Std score of good states: 833.815431, bad states: 1088.622675
Min score of good states: -14390.529271, bad states: -46878.843825
Max score of good states: 594.626310, bad states: 583.422610
Bad percentiles:
0 percentile: -46878.843825
10 percentile: 321.806538
20 percentile: 403.110250
30 percentile: 435.417315
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -14390.529271
10 percentile: 279.324501
20 percentile: 368.791343
30 percentile: 398.211970
40 percentile: 420.713409
50 percentile: 437.925106
60 percentile: 452.170003
70 percentile: 475.496151
80 percentile: 520.203371
90 percentile: 583.422610
100 percentile: 594.626310

======================================================

Generating 100 trajectories for policy iter0000280...
Calculating scores...
Dataset size: 12455 transitions (100 trajectories)
Average return: 835.653447817
<type 'numpy.ndarray'>
Dataset size: 12455 transitions (100 trajectories)
Average return: 835.653447817
<type 'numpy.ndarray'>
upper_bound_reward:  1239.41890048
Mean score of good states: 356.648378, bad states: 511.479165
Std score of good states: 1567.268330, bad states: 202.533282
Min score of good states: -60643.903436, bad states: -7197.205851
Max score of good states: 588.557949, bad states: 583.422610
Bad percentiles:
0 percentile: -7197.205851
10 percentile: 381.288269
20 percentile: 427.352933
30 percentile: 462.882548
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -60643.903436
10 percentile: 299.112666
20 percentile: 371.894747
30 percentile: 407.564670
40 percentile: 431.394302
50 percentile: 449.358497
60 percentile: 475.145736
70 percentile: 523.280261
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 588.557949

======================================================

Generating 100 trajectories for policy iter0000300...
Calculating scores...
Dataset size: 13916 transitions (100 trajectories)
Average return: 975.87854421
<type 'numpy.ndarray'>
Dataset size: 13916 transitions (100 trajectories)
Average return: 975.87854421
<type 'numpy.ndarray'>
upper_bound_reward:  1497.7186336
Mean score of good states: -1417.993729, bad states: 506.153732
Std score of good states: 57055.140150, bad states: 142.131606
Min score of good states: -2181508.815826, bad states: -771.235576
Max score of good states: 599.206650, bad states: 583.422610
Bad percentiles:
0 percentile: -771.235576
10 percentile: 345.477717
20 percentile: 415.327005
30 percentile: 452.086825
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -2181508.815826
10 percentile: 327.622993
20 percentile: 385.846243
30 percentile: 413.295323
40 percentile: 434.370664
50 percentile: 452.424524
60 percentile: 479.534818
70 percentile: 512.751234
80 percentile: 540.230363
90 percentile: 583.422610
100 percentile: 599.206650

======================================================

Generating 100 trajectorieLoading policy parameters from /snapshots/iter0000320 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000320
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 1100.95832176
Loading policy parameters from /snapshots/iter0000340 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000340
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 1311.56984016
Loading policy parameters from /snapshots/iter0000360 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000360
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 1460.84662543
Loading policy parameters from /snapshots/iter0000380 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000380
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 1644.14323434
s for policy iter0000320...
Calculating scores...
Dataset size: 15135 transitions (100 trajectories)
Average return: 1100.95832176
<type 'numpy.ndarray'>
Dataset size: 15135 transitions (100 trajectories)
Average return: 1100.95832176
<type 'numpy.ndarray'>
upper_bound_reward:  1657.86452879
Mean score of good states: 343.277507, bad states: 501.879902
Std score of good states: 4422.632569, bad states: 600.003711
Min score of good states: -227219.478359, bad states: -22874.370018
Max score of good states: 607.844598, bad states: 583.422610
Bad percentiles:
0 percentile: -22874.370018
10 percentile: 371.964186
20 percentile: 428.718593
30 percentile: 475.380539
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -227219.478359
10 percentile: 328.200738
20 percentile: 394.047592
30 percentile: 423.161823
40 percentile: 444.035543
50 percentile: 464.807147
60 percentile: 499.817606
70 percentile: 532.478649
80 percentile: 565.246828
90 percentile: 583.422610
100 percentile: 607.844598

======================================================

Generating 100 trajectories for policy iter0000340...
Calculating scores...
Dataset size: 17339 transitions (100 trajectories)
Average return: 1311.56984016
<type 'numpy.ndarray'>
Dataset size: 17339 transitions (100 trajectories)
Average return: 1311.56984016
<type 'numpy.ndarray'>
upper_bound_reward:  1941.89837023
Mean score of good states: 218.373237, bad states: 518.193309
Std score of good states: 13668.530447, bad states: 320.909778
Min score of good states: -806635.800439, bad states: -12334.434327
Max score of good states: 608.788340, bad states: 583.422610
Bad percentiles:
0 percentile: -12334.434327
10 percentile: 389.109238
20 percentile: 435.947123
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -806635.800439
10 percentile: 349.224697
20 percentile: 409.201746
30 percentile: 436.277566
40 percentile: 458.878510
50 percentile: 494.287527
60 percentile: 521.400589
70 percentile: 544.751908
80 percentile: 579.471055
90 percentile: 583.422610
100 percentile: 608.788340

======================================================

Generating 100 trajectories for policy iter0000360...
Calculating scores...
Dataset size: 18743 transitions (100 trajectories)
Average return: 1460.84662543
<type 'numpy.ndarray'>
Dataset size: 18743 transitions (100 trajectories)
Average return: 1460.84662543
<type 'numpy.ndarray'>
upper_bound_reward:  2281.0231989
Mean score of good states: 448.433727, bad states: 523.845573
Std score of good states: 425.712654, bad states: 244.971422
Min score of good states: -12013.587788, bad states: -12343.925932
Max score of good states: 604.901982, bad states: 583.422610
Bad percentiles:
0 percentile: -12343.925932
10 percentile: 395.993369
20 percentile: 444.284979
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -12013.587788
10 percentile: 345.115914
20 percentile: 406.439982
30 percentile: 433.398570
40 percentile: 454.384685
50 percentile: 482.249836
60 percentile: 510.433791
70 percentile: 535.756593
80 percentile: 559.940967
90 percentile: 583.422610
100 percentile: 604.901982

======================================================

Generating 100 trajectories for policy iter0000380...
Calculating scores...
Dataset size: 20442 transitions (100 trajectories)
Average return: 1644.14323434
<type 'numpy.ndarray'>
Dataset size: 20442 transitions (100 trajectories)
Average return: 1644.14323434
<type 'numpy.ndarray'>
upper_bound_reward:  2455.83812901
Mean score of good states: 413.293250, bad states: 542.989433
Std score of goLoading policy parameters from /snapshots/iter0000400 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000400
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 1928.64630588
Loading policy parameters from /snapshots/iter0000420 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000420
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 3007.68148072
Loading policy parameters from /snapshots/iter0000440 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000440
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 4005.59221645
od states: 3681.704957, bad states: 89.313368
Min score of good states: -246716.678603, bad states: -624.004411
Max score of good states: 610.512017, bad states: 590.014105
Bad percentiles:
0 percentile: -624.004411
10 percentile: 419.908161
20 percentile: 485.672340
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 590.014105
Good percentiles:
0 percentile: -246716.678603
10 percentile: 356.869706
20 percentile: 417.801886
30 percentile: 449.821318
40 percentile: 484.847483
50 percentile: 516.586576
60 percentile: 543.619067
70 percentile: 576.064233
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 610.512017

======================================================

Generating 100 trajectories for policy iter0000400...
Calculating scores...
Dataset size: 23180 transitions (100 trajectories)
Average return: 1928.64630588
<type 'numpy.ndarray'>
Dataset size: 23180 transitions (100 trajectories)
Average return: 1928.64630588
<type 'numpy.ndarray'>
upper_bound_reward:  2898.29378701
Mean score of good states: 504.973697, bad states: 545.179612
Std score of good states: 286.402538, bad states: 118.681693
Min score of good states: -11489.662489, bad states: -1749.932223
Max score of good states: 606.366683, bad states: 604.119595
Bad percentiles:
0 percentile: -1749.932223
10 percentile: 434.335162
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 604.119595
Good percentiles:
0 percentile: -11489.662489
10 percentile: 398.581365
20 percentile: 450.074186
30 percentile: 492.489621
40 percentile: 526.294142
50 percentile: 554.711692
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 606.366683

======================================================

Generating 100 trajectories for policy iter0000420...
Calculating scores...
Dataset size: 33357 transitions (100 trajectories)
Average return: 3007.68148072
<type 'numpy.ndarray'>
Dataset size: 33357 transitions (100 trajectories)
Average return: 3007.68148072
<type 'numpy.ndarray'>
upper_bound_reward:  5048.80198116
Mean score of good states: 476.725768, bad states: 353.867108
Std score of good states: 2188.312095, bad states: 20038.869085
Min score of good states: -211668.785571, bad states: -2002815.259681
Max score of good states: 614.425709, bad states: 596.803819
Bad percentiles:
0 percentile: -2002815.259681
10 percentile: 463.005372
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 596.803819
Good percentiles:
0 percentile: -211668.785571
10 percentile: 399.074336
20 percentile: 459.040272
30 percentile: 503.704370
40 percentile: 533.080878
50 percentile: 554.946735
60 percentile: 575.749947
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 614.425709

======================================================

Generating 100 trajectories for policy iter0000440...
Calculating scores...
Dataset size: 42801 transitions (100 trajectories)
Average return: 4005.59221645
<type 'numpy.ndarray'>
Dataset size: 42801 transitions (100 trajectories)
Average return: 4005.59221645
<type 'numpy.ndarray'>
upper_bound_reward:  7365.43060382
Mean score of good states: 483.047778, bad states: 549.452762
Std score of good states: 434.124777, bad states: 357.825844
Min score of good states: -19436.005629, bad states: -17211.941470
Max score of good states: 616.883427, bad states: 588.967081
Bad percentiles:
0 percentile: -17211.941470
10 percentile: 461.701983
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422Loading policy parameters from /snapshots/iter0000460 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000460
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 4983.31148679
Loading policy parameters from /snapshots/iter0000480 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000480
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 6837.54972154
Loading policy parameters from /snapshots/iter0000500 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000500
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 7335.23879906
610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 588.967081
Good percentiles:
0 percentile: -19436.005629
10 percentile: 383.642586
20 percentile: 445.624263
30 percentile: 481.020879
40 percentile: 508.606772
50 percentile: 528.394241
60 percentile: 544.097709
70 percentile: 556.818665
80 percentile: 570.348161
90 percentile: 583.422610
100 percentile: 616.883427

======================================================

Generating 100 trajectories for policy iter0000460...
Calculating scores...
Dataset size: 51706 transitions (100 trajectories)
Average return: 4983.31148679
<type 'numpy.ndarray'>
Dataset size: 51706 transitions (100 trajectories)
Average return: 4983.31148679
<type 'numpy.ndarray'>
upper_bound_reward:  10117.1344172
Mean score of good states: 468.552328, bad states: 548.115911
Std score of good states: 2352.063477, bad states: 404.585206
Min score of good states: -231255.103987, bad states: -32618.516791
Max score of good states: 617.361972, bad states: 583.422610
Bad percentiles:
0 percentile: -32618.516791
10 percentile: 453.842505
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -231255.103987
10 percentile: 393.917979
20 percentile: 455.156952
30 percentile: 496.955292
40 percentile: 520.703711
50 percentile: 535.612930
60 percentile: 547.810893
70 percentile: 558.661257
80 percentile: 569.734852
90 percentile: 582.358055
100 percentile: 617.361972

======================================================

Generating 100 trajectories for policy iter0000480...
Calculating scores...
Dataset size: 69196 transitions (100 trajectories)
Average return: 6837.54972154
<type 'numpy.ndarray'>
Dataset size: 69196 transitions (100 trajectories)
Average return: 6837.54972154
<type 'numpy.ndarray'>
upper_bound_reward:  10165.339787
Mean score of good states: 501.705686, bad states: 546.399179
Std score of good states: 267.914119, bad states: 508.583429
Min score of good states: -13138.863094, bad states: -41876.018398
Max score of good states: 619.195252, bad states: 583.422610
Bad percentiles:
0 percentile: -41876.018398
10 percentile: 455.165803
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -13138.863094
10 percentile: 398.892172
20 percentile: 459.589918
30 percentile: 497.275446
40 percentile: 520.700348
50 percentile: 536.860114
60 percentile: 550.179575
70 percentile: 560.679782
80 percentile: 571.522268
90 percentile: 582.977171
100 percentile: 619.195252

======================================================

Generating 100 trajectories for policy iter0000500...
Calculating scores...
Dataset size: 73764 transitions (100 trajectories)
Average return: 7335.23879906
<type 'numpy.ndarray'>
Dataset size: 73764 transitions (100 trajectories)
Average return: 7335.23879906
<type 'numpy.ndarray'>
upper_bound_reward:  10184.3499653
Mean score of good states: 505.793401, bad states: 550.131742
Std score of good states: 228.371066, bad states: 407.870425
Min score of good states: -11365.523448, bad states: -32622.470995
Max score of good states: 618.887672, bad states: 583.422610
Bad percentiles:
0 percentile: -32622.470995
10 percentile: 460.362154
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -11365.523448
10 percentile: 390.743302
20 percentile: 461.471384
30 percentile: 502.329918
40 percentile: 525.275986
50 percentile: 540.458870
60 percentile: 552.736360
70 percentile: 563.272329
80 percentile: 573.226344
90 percentiLoading policy parameters from /snapshots/iter0000520 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000520
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 7993.53495081
Loading policy parameters from /snapshots/iter0000540 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000540
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 7978.68980499
Loading policy parameters from /snapshots/iter0000560 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000560
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 7953.92556088
Loading policy parameters from /snapshots/iter0000580 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000580
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8242.41274885
le: 584.467359
100 percentile: 618.887672

======================================================

Generating 100 trajectories for policy iter0000520...
Calculating scores...
Dataset size: 79829 transitions (100 trajectories)
Average return: 7993.53495081
<type 'numpy.ndarray'>
Dataset size: 79829 transitions (100 trajectories)
Average return: 7993.53495081
<type 'numpy.ndarray'>
upper_bound_reward:  10226.2817919
Mean score of good states: 505.330166, bad states: -686.866475
Std score of good states: 350.568416, bad states: 117962.916887
Min score of good states: -15635.841600, bad states: -11777512.879012
Max score of good states: 620.246789, bad states: 593.450270
Bad percentiles:
0 percentile: -11777512.879012
10 percentile: 467.169154
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 593.450270
Good percentiles:
0 percentile: -15635.841600
10 percentile: 407.015414
20 percentile: 470.077514
30 percentile: 506.559194
40 percentile: 528.763940
50 percentile: 543.734854
60 percentile: 555.191381
70 percentile: 565.158212
80 percentile: 574.393578
90 percentile: 584.584230
100 percentile: 620.246789

======================================================

Generating 100 trajectories for policy iter0000540...
Calculating scores...
Dataset size: 79896 transitions (100 trajectories)
Average return: 7978.68980499
<type 'numpy.ndarray'>
Dataset size: 79896 transitions (100 trajectories)
Average return: 7978.68980499
<type 'numpy.ndarray'>
upper_bound_reward:  10194.3270138
Mean score of good states: 506.060401, bad states: 531.246513
Std score of good states: 337.345841, bad states: 1975.672936
Min score of good states: -18007.799833, bad states: -195860.440363
Max score of good states: 620.602507, bad states: 602.749157
Bad percentiles:
0 percentile: -195860.440363
10 percentile: 449.841407
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 602.749157
Good percentiles:
0 percentile: -18007.799833
10 percentile: 396.991197
20 percentile: 469.574929
30 percentile: 508.927075
40 percentile: 530.063393
50 percentile: 544.993595
60 percentile: 556.229385
70 percentile: 565.895243
80 percentile: 575.261256
90 percentile: 585.367530
100 percentile: 620.602507

======================================================

Generating 100 trajectories for policy iter0000560...
Calculating scores...
Dataset size: 79840 transitions (100 trajectories)
Average return: 7953.92556088
<type 'numpy.ndarray'>
Dataset size: 79840 transitions (100 trajectories)
Average return: 7953.92556088
<type 'numpy.ndarray'>
upper_bound_reward:  10194.081103
Mean score of good states: 509.949075, bad states: 495.159564
Std score of good states: 287.790552, bad states: 2017.866073
Min score of good states: -15040.668947, bad states: -158462.173001
Max score of good states: 620.352230, bad states: 605.926022
Bad percentiles:
0 percentile: -158462.173001
10 percentile: 446.917158
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 605.926022
Good percentiles:
0 percentile: -15040.668947
10 percentile: 405.687857
20 percentile: 467.095893
30 percentile: 509.089844
40 percentile: 532.454785
50 percentile: 546.721027
60 percentile: 557.586699
70 percentile: 566.948375
80 percentile: 576.253112
90 percentile: 585.943434
100 percentile: 620.352230

======================================================

Generating 100 trajectories for policy iter0000580...
Calculating scores...
Dataset size: 82294 transitions (100 trajectories)
Average return: 8242.41274885
<type 'numpy.ndarray'>
Dataset size: 82294 transitions (100 trajectories)
Average return: 8242.41274885
<Loading policy parameters from /snapshots/iter0000600 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000600
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8588.49083614
Loading policy parameters from /snapshots/iter0000620 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000620
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8525.36209719
Loading policy parameters from /snapshots/iter0000640 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000640
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8250.09148004
type 'numpy.ndarray'>
upper_bound_reward:  10220.9789687
Mean score of good states: 507.931721, bad states: 543.208031
Std score of good states: 254.543207, bad states: 786.253374
Min score of good states: -14523.636124, bad states: -66273.191715
Max score of good states: 629.603376, bad states: 602.429145
Bad percentiles:
0 percentile: -66273.191715
10 percentile: 463.660753
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 602.429145
Good percentiles:
0 percentile: -14523.636124
10 percentile: 402.012313
20 percentile: 467.072290
30 percentile: 504.858973
40 percentile: 528.216863
50 percentile: 543.278743
60 percentile: 554.515069
70 percentile: 564.044952
80 percentile: 573.829526
90 percentile: 584.265374
100 percentile: 629.603376

======================================================

Generating 100 trajectories for policy iter0000600...
Calculating scores...
Dataset size: 85809 transitions (100 trajectories)
Average return: 8588.49083614
<type 'numpy.ndarray'>
Dataset size: 85809 transitions (100 trajectories)
Average return: 8588.49083614
<type 'numpy.ndarray'>
upper_bound_reward:  10172.278318
Mean score of good states: 514.004320, bad states: 499.768456
Std score of good states: 177.293022, bad states: 2611.464039
Min score of good states: -9712.666956, bad states: -216880.712729
Max score of good states: 622.996079, bad states: 593.632672
Bad percentiles:
0 percentile: -216880.712729
10 percentile: 444.056665
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 593.632672
Good percentiles:
0 percentile: -9712.666956
10 percentile: 411.973078
20 percentile: 471.299242
30 percentile: 508.430034
40 percentile: 530.635538
50 percentile: 545.166160
60 percentile: 555.951816
70 percentile: 565.864678
80 percentile: 575.097501
90 percentile: 586.311133
100 percentile: 622.996079

======================================================

Generating 100 trajectories for policy iter0000620...
Calculating scores...
Dataset size: 85309 transitions (100 trajectories)
Average return: 8525.36209719
<type 'numpy.ndarray'>
Dataset size: 85309 transitions (100 trajectories)
Average return: 8525.36209719
<type 'numpy.ndarray'>
upper_bound_reward:  10159.6267352
Mean score of good states: 519.226975, bad states: 110.601689
Std score of good states: 125.873291, bad states: 39932.551424
Min score of good states: -9151.760093, bad states: -3981315.135608
Max score of good states: 622.188496, bad states: 583.422610
Bad percentiles:
0 percentile: -3981315.135608
10 percentile: 448.682981
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -9151.760093
10 percentile: 421.454398
20 percentile: 476.974672
30 percentile: 511.655028
40 percentile: 532.570737
50 percentile: 546.869807
60 percentile: 557.882644
70 percentile: 567.438698
80 percentile: 576.280292
90 percentile: 586.400078
100 percentile: 622.188496

======================================================

Generating 100 trajectories for policy iter0000640...
Calculating scores...
Dataset size: 82904 transitions (100 trajectories)
Average return: 8250.09148004
<type 'numpy.ndarray'>
Dataset size: 82904 transitions (100 trajectories)
Average return: 8250.09148004
<type 'numpy.ndarray'>
upper_bound_reward:  10153.4762373
Mean score of good states: 509.061578, bad states: 463.854084
Std score of good states: 442.762505, bad states: 8782.407051
Min score of good states: -38500.061230, bad states: -876857.605817
Max score of good states: 623.346045, bad states: 583.422610
Bad percentiles:
0 percentile: -876857.605817
10 percentile: 4Loading policy parameters from /snapshots/iter0000660 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000660
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8873.28623465
Loading policy parameters from /snapshots/iter0000680 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000680
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8804.35066008
Loading policy parameters from /snapshots/iter0000700 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000700
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8058.13258365
67.062136
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -38500.061230
10 percentile: 408.805903
20 percentile: 470.826268
30 percentile: 509.033911
40 percentile: 531.115062
50 percentile: 545.217059
60 percentile: 556.476882
70 percentile: 566.456161
80 percentile: 575.885912
90 percentile: 585.331243
100 percentile: 623.346045

======================================================

Generating 100 trajectories for policy iter0000660...
Calculating scores...
Dataset size: 88957 transitions (100 trajectories)
Average return: 8873.28623465
<type 'numpy.ndarray'>
Dataset size: 88957 transitions (100 trajectories)
Average return: 8873.28623465
<type 'numpy.ndarray'>
upper_bound_reward:  10133.5100181
Mean score of good states: 514.863072, bad states: 530.309849
Std score of good states: 177.571269, bad states: 519.515742
Min score of good states: -11748.425518, bad states: -30004.214416
Max score of good states: 618.856609, bad states: 600.654674
Bad percentiles:
0 percentile: -30004.214416
10 percentile: 434.901906
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 600.654674
Good percentiles:
0 percentile: -11748.425518
10 percentile: 419.202116
20 percentile: 471.654471
30 percentile: 508.185914
40 percentile: 528.958254
50 percentile: 542.983844
60 percentile: 554.581926
70 percentile: 564.803057
80 percentile: 574.697532
90 percentile: 585.376819
100 percentile: 618.856609

======================================================

Generating 100 trajectories for policy iter0000680...
Calculating scores...
Dataset size: 88017 transitions (100 trajectories)
Average return: 8804.35066008
<type 'numpy.ndarray'>
Dataset size: 88017 transitions (100 trajectories)
Average return: 8804.35066008
<type 'numpy.ndarray'>
upper_bound_reward:  10170.8435349
Mean score of good states: 508.659974, bad states: 381.572379
Std score of good states: 238.195529, bad states: 12847.625085
Min score of good states: -13154.712237, bad states: -1227648.359496
Max score of good states: 624.046130, bad states: 588.644125
Bad percentiles:
0 percentile: -1227648.359496
10 percentile: 440.628929
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 588.644125
Good percentiles:
0 percentile: -13154.712237
10 percentile: 403.821521
20 percentile: 464.997066
30 percentile: 502.846738
40 percentile: 526.168798
50 percentile: 541.348142
60 percentile: 553.265533
70 percentile: 562.793630
80 percentile: 572.376424
90 percentile: 583.580643
100 percentile: 624.046130

======================================================

Generating 100 trajectories for policy iter0000700...
Calculating scores...
Dataset size: 81088 transitions (100 trajectories)
Average return: 8058.13258365
<type 'numpy.ndarray'>
Dataset size: 81088 transitions (100 trajectories)
Average return: 8058.13258365
<type 'numpy.ndarray'>
upper_bound_reward:  10144.3482357
Mean score of good states: 515.234882, bad states: 547.347874
Std score of good states: 207.357904, bad states: 381.719091
Min score of good states: -11747.942471, bad states: -13578.459886
Max score of good states: 620.857737, bad states: 605.694933
Bad percentiles:
0 percentile: -13578.459886
10 percentile: 460.287690
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 605.694933
Good percentiles:
0 percentile: -11747.942471
10 percentile: 424.660651
20 percentile: 476.581763
30 percentile: 510.571659
4Loading policy parameters from /snapshots/iter0000720 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000720
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9054.50504704
Loading policy parameters from /snapshots/iter0000740 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000740
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8892.80332193
Loading policy parameters from /snapshots/iter0000760 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000760
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9259.0213283
Loading policy parameters from /snapshots/iter0000780 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000780
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9019.31557619
0 percentile: 530.526226
50 percentile: 544.245653
60 percentile: 555.735158
70 percentile: 565.246408
80 percentile: 574.525959
90 percentile: 584.895325
100 percentile: 620.857737

======================================================

Generating 100 trajectories for policy iter0000720...
Calculating scores...
Dataset size: 90412 transitions (100 trajectories)
Average return: 9054.50504704
<type 'numpy.ndarray'>
Dataset size: 90412 transitions (100 trajectories)
Average return: 9054.50504704
<type 'numpy.ndarray'>
upper_bound_reward:  10180.0815138
Mean score of good states: 487.619709, bad states: 532.153849
Std score of good states: 2325.559627, bad states: 280.417102
Min score of good states: -229017.899491, bad states: -11804.927182
Max score of good states: 620.531098, bad states: 609.226191
Bad percentiles:
0 percentile: -11804.927182
10 percentile: 423.330778
20 percentile: 522.787859
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 609.226191
Good percentiles:
0 percentile: -229017.899491
10 percentile: 409.938457
20 percentile: 472.442330
30 percentile: 510.316060
40 percentile: 533.007233
50 percentile: 546.275354
60 percentile: 556.891890
70 percentile: 566.520180
80 percentile: 575.483349
90 percentile: 585.532113
100 percentile: 620.531098

======================================================

Generating 100 trajectories for policy iter0000740...
Calculating scores...
Dataset size: 88871 transitions (100 trajectories)
Average return: 8892.80332193
<type 'numpy.ndarray'>
Dataset size: 88871 transitions (100 trajectories)
Average return: 8892.80332193
<type 'numpy.ndarray'>
upper_bound_reward:  10181.1600277
Mean score of good states: 521.035857, bad states: 532.176968
Std score of good states: 163.631545, bad states: 891.437803
Min score of good states: -13553.788754, bad states: -84103.355177
Max score of good states: 623.179209, bad states: 611.987545
Bad percentiles:
0 percentile: -84103.355177
10 percentile: 438.122476
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 611.987545
Good percentiles:
0 percentile: -13553.788754
10 percentile: 419.319500
20 percentile: 480.611507
30 percentile: 517.810051
40 percentile: 537.084026
50 percentile: 550.701491
60 percentile: 560.983304
70 percentile: 569.708170
80 percentile: 578.351032
90 percentile: 588.394371
100 percentile: 623.179209

======================================================

Generating 100 trajectories for policy iter0000760...
Calculating scores...
Dataset size: 92515 transitions (100 trajectories)
Average return: 9259.0213283
<type 'numpy.ndarray'>
Dataset size: 92515 transitions (100 trajectories)
Average return: 9259.0213283
<type 'numpy.ndarray'>
upper_bound_reward:  10139.5982014
Mean score of good states: 515.661157, bad states: 532.519842
Std score of good states: 190.701902, bad states: 350.946240
Min score of good states: -13810.875380, bad states: -16364.725154
Max score of good states: 620.415581, bad states: 611.207925
Bad percentiles:
0 percentile: -16364.725154
10 percentile: 440.204924
20 percentile: 503.940143
30 percentile: 551.319211
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 611.207925
Good percentiles:
0 percentile: -13810.875380
10 percentile: 421.533810
20 percentile: 474.058530
30 percentile: 509.299214
40 percentile: 530.527296
50 percentile: 545.199826
60 percentile: 555.491437
70 percentile: 564.876710
80 percentile: 574.223697
90 percentile: 584.615495
100 percentile: 620.415581

======================================================

Generating 100 trajectories for policy iter0000780...
Calculating scores...
Dataset size: 90061 transitions (100 trajectories)
Average rLoading policy parameters from /snapshots/iter0000800 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000800
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8860.65090416
Loading policy parameters from /snapshots/iter0000820 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000820
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9299.3580691
Loading policy parameters from /snapshots/iter0000840 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000840
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8979.41154451
eturn: 9019.31557619
<type 'numpy.ndarray'>
Dataset size: 90061 transitions (100 trajectories)
Average return: 9019.31557619
<type 'numpy.ndarray'>
upper_bound_reward:  10165.5757917
Mean score of good states: 515.973986, bad states: 535.122139
Std score of good states: 207.370476, bad states: 389.604477
Min score of good states: -11261.192632, bad states: -18633.366658
Max score of good states: 620.713663, bad states: 605.899968
Bad percentiles:
0 percentile: -18633.366658
10 percentile: 433.496392
20 percentile: 550.850180
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 605.899968
Good percentiles:
0 percentile: -11261.192632
10 percentile: 418.368066
20 percentile: 475.920521
30 percentile: 511.231150
40 percentile: 532.824616
50 percentile: 546.406061
60 percentile: 557.571366
70 percentile: 567.046576
80 percentile: 576.272789
90 percentile: 586.229478
100 percentile: 620.713663

======================================================

Generating 100 trajectories for policy iter0000800...
Calculating scores...
Dataset size: 88445 transitions (100 trajectories)
Average return: 8860.65090416
<type 'numpy.ndarray'>
Dataset size: 88445 transitions (100 trajectories)
Average return: 8860.65090416
<type 'numpy.ndarray'>
upper_bound_reward:  10188.5119706
Mean score of good states: 491.740089, bad states: -5278.797805
Std score of good states: 2665.554405, bad states: 420369.002917
Min score of good states: -265319.321657, bad states: -39012596.195204
Max score of good states: 623.860401, bad states: 597.139091
Bad percentiles:
0 percentile: -39012596.195204
10 percentile: 427.322988
20 percentile: 574.077888
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 597.139091
Good percentiles:
0 percentile: -265319.321657
10 percentile: 417.941669
20 percentile: 477.357318
30 percentile: 513.942887
40 percentile: 534.690504
50 percentile: 548.628947
60 percentile: 559.707373
70 percentile: 568.641589
80 percentile: 577.312677
90 percentile: 587.354390
100 percentile: 623.860401

======================================================

Generating 100 trajectories for policy iter0000820...
Calculating scores...
Dataset size: 92653 transitions (100 trajectories)
Average return: 9299.3580691
<type 'numpy.ndarray'>
Dataset size: 92653 transitions (100 trajectories)
Average return: 9299.3580691
<type 'numpy.ndarray'>
upper_bound_reward:  10201.7004134
Mean score of good states: -1919.065937, bad states: 361.468566
Std score of good states: 243705.638819, bad states: 11413.677022
Min score of good states: -24371258.474671, bad states: -961985.995889
Max score of good states: 621.249840, bad states: 614.712827
Bad percentiles:
0 percentile: -961985.995889
10 percentile: 416.754315
20 percentile: 502.408660
30 percentile: 573.336998
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 614.712827
Good percentiles:
0 percentile: -24371258.474671
10 percentile: 417.629848
20 percentile: 475.106605
30 percentile: 512.648123
40 percentile: 534.545519
50 percentile: 548.752595
60 percentile: 559.311014
70 percentile: 568.842581
80 percentile: 577.761324
90 percentile: 587.267943
100 percentile: 621.249840

======================================================

Generating 100 trajectories for policy iter0000840...
Calculating scores...
Dataset size: 89623 transitions (100 trajectories)
Average return: 8979.41154451
<type 'numpy.ndarray'>
Dataset size: 89623 transitions (100 trajectories)
Average return: 8979.41154451
<type 'numpy.ndarray'>
upper_bound_reward:  10172.9958976
Mean score of good states: 510.498369, bad states: -1499.426293
Std score of good states: 558.938103, bad states: 143870.890806
Min score of good states: -46344.71785Loading policy parameters from /snapshots/iter0000860 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000860
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8915.17710992
Loading policy parameters from /snapshots/iter0000880 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000880
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9201.42221508
Loading policy parameters from /snapshots/iter0000900 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000900
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9019.67204601
3, bad states: -11591738.734170
Max score of good states: 622.289206, bad states: 607.829667
Bad percentiles:
0 percentile: -11591738.734170
10 percentile: 431.086015
20 percentile: 562.574274
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 607.829667
Good percentiles:
0 percentile: -46344.717853
10 percentile: 424.569347
20 percentile: 479.267985
30 percentile: 514.587893
40 percentile: 534.276913
50 percentile: 548.164618
60 percentile: 559.087226
70 percentile: 568.309340
80 percentile: 577.155642
90 percentile: 586.899257
100 percentile: 622.289206

======================================================

Generating 100 trajectories for policy iter0000860...
Calculating scores...
Dataset size: 89082 transitions (100 trajectories)
Average return: 8915.17710992
<type 'numpy.ndarray'>
Dataset size: 89082 transitions (100 trajectories)
Average return: 8915.17710992
<type 'numpy.ndarray'>
upper_bound_reward:  10179.4524573
Mean score of good states: 511.456117, bad states: 541.040983
Std score of good states: 273.352197, bad states: 389.339836
Min score of good states: -15433.368453, bad states: -20300.025366
Max score of good states: 621.210830, bad states: 598.525129
Bad percentiles:
0 percentile: -20300.025366
10 percentile: 439.916232
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 598.525129
Good percentiles:
0 percentile: -15433.368453
10 percentile: 418.597895
20 percentile: 471.839100
30 percentile: 505.620119
40 percentile: 529.184245
50 percentile: 544.279231
60 percentile: 555.939837
70 percentile: 566.313957
80 percentile: 575.798367
90 percentile: 585.832476
100 percentile: 621.210830

======================================================

Generating 100 trajectories for policy iter0000880...
Calculating scores...
Dataset size: 91646 transitions (100 trajectories)
Average return: 9201.42221508
<type 'numpy.ndarray'>
Dataset size: 91646 transitions (100 trajectories)
Average return: 9201.42221508
<type 'numpy.ndarray'>
upper_bound_reward:  10189.9887615
Mean score of good states: 515.366703, bad states: 530.197437
Std score of good states: 194.582825, bad states: 379.874102
Min score of good states: -12542.571680, bad states: -19793.465541
Max score of good states: 619.745388, bad states: 610.752367
Bad percentiles:
0 percentile: -19793.465541
10 percentile: 416.532740
20 percentile: 512.329790
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 610.752367
Good percentiles:
0 percentile: -12542.571680
10 percentile: 414.331769
20 percentile: 472.312422
30 percentile: 510.322047
40 percentile: 531.523866
50 percentile: 545.036112
60 percentile: 556.487422
70 percentile: 566.105745
80 percentile: 575.436724
90 percentile: 586.013958
100 percentile: 619.745388

======================================================

Generating 100 trajectories for policy iter0000900...
Calculating scores...
Dataset size: 89843 transitions (100 trajectories)
Average return: 9019.67204601
<type 'numpy.ndarray'>
Dataset size: 89843 transitions (100 trajectories)
Average return: 9019.67204601
<type 'numpy.ndarray'>
upper_bound_reward:  10182.0030573
Mean score of good states: 514.661605, bad states: -3705.180678
Std score of good states: 398.603756, bad states: 382710.515119
Min score of good states: -31648.481391, bad states: -38089832.786930
Max score of good states: 619.456316, bad states: 618.937977
Bad percentiles:
0 percentile: -38089832.786930
10 percentile: 455.638217
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 5Loading policy parameters from /snapshots/iter0000920 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000920
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9348.92949013
Loading policy parameters from /snapshots/iter0000940 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000940
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8956.95953767
Loading policy parameters from /snapshots/iter0000960 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000960
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8685.6661959
83.422610
100 percentile: 618.937977
Good percentiles:
0 percentile: -31648.481391
10 percentile: 428.866993
20 percentile: 478.918616
30 percentile: 512.909805
40 percentile: 532.629051
50 percentile: 547.265576
60 percentile: 557.622435
70 percentile: 567.131995
80 percentile: 576.664811
90 percentile: 586.715978
100 percentile: 619.456316

======================================================

Generating 100 trajectories for policy iter0000920...
Calculating scores...
Dataset size: 93082 transitions (100 trajectories)
Average return: 9348.92949013
<type 'numpy.ndarray'>
Dataset size: 93082 transitions (100 trajectories)
Average return: 9348.92949013
<type 'numpy.ndarray'>
upper_bound_reward:  10171.2449683
Mean score of good states: 513.093856, bad states: 519.617261
Std score of good states: 286.826650, bad states: 578.454338
Min score of good states: -13716.844112, bad states: -34362.315022
Max score of good states: 621.610919, bad states: 619.627464
Bad percentiles:
0 percentile: -34362.315022
10 percentile: 422.577180
20 percentile: 500.504245
30 percentile: 562.930503
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 619.627464
Good percentiles:
0 percentile: -13716.844112
10 percentile: 418.538732
20 percentile: 475.326983
30 percentile: 510.673940
40 percentile: 531.557272
50 percentile: 545.717261
60 percentile: 556.649067
70 percentile: 566.262105
80 percentile: 575.926262
90 percentile: 586.423902
100 percentile: 621.610919

======================================================

Generating 100 trajectories for policy iter0000940...
Calculating scores...
Dataset size: 89377 transitions (100 trajectories)
Average return: 8956.95953767
<type 'numpy.ndarray'>
Dataset size: 89377 transitions (100 trajectories)
Average return: 8956.95953767
<type 'numpy.ndarray'>
upper_bound_reward:  10166.5976051
Mean score of good states: 509.720723, bad states: 498.171528
Std score of good states: 327.647265, bad states: 4203.114673
Min score of good states: -13494.870905, bad states: -413402.553822
Max score of good states: 624.150468, bad states: 608.744019
Bad percentiles:
0 percentile: -413402.553822
10 percentile: 449.825404
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 608.744019
Good percentiles:
0 percentile: -13494.870905
10 percentile: 417.142056
20 percentile: 472.463640
30 percentile: 508.856780
40 percentile: 528.765949
50 percentile: 544.089042
60 percentile: 555.910059
70 percentile: 565.757082
80 percentile: 575.480419
90 percentile: 586.251976
100 percentile: 624.150468

======================================================

Generating 100 trajectories for policy iter0000960...
Calculating scores...
Dataset size: 86901 transitions (100 trajectories)
Average return: 8685.6661959
<type 'numpy.ndarray'>
Dataset size: 86901 transitions (100 trajectories)
Average return: 8685.6661959
<type 'numpy.ndarray'>
upper_bound_reward:  10167.968359
Mean score of good states: 522.237939, bad states: 537.533241
Std score of good states: 187.891444, bad states: 396.588504
Min score of good states: -11589.196056, bad states: -14157.460101
Max score of good states: 624.539577, bad states: 583.422610
Bad percentiles:
0 percentile: -14157.460101
10 percentile: 443.408668
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -11589.196056
10 percentile: 430.734605
20 percentile: 483.007469
30 percentile: 516.512225
40 percentile: 536.691662
50 percentile: 550.313521
60 percentile: 560.966191
70 percentile: 570.071963
80 percentile: 578.969700
90 percentile: 588.709111
100 percentile: 624.539577

===============================Loading policy parameters from /snapshots/iter0000980 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0000980
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9103.19751208
Loading policy parameters from /snapshots/iter0001000 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001000
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9310.87323427
Loading policy parameters from /snapshots/iter0001020 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001020
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9384.63238594
Loading policy parameters from /snapshots/iter0001040 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001040
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9023.49998409
=======================

Generating 100 trajectories for policy iter0000980...
Calculating scores...
Dataset size: 90950 transitions (100 trajectories)
Average return: 9103.19751208
<type 'numpy.ndarray'>
Dataset size: 90950 transitions (100 trajectories)
Average return: 9103.19751208
<type 'numpy.ndarray'>
upper_bound_reward:  10164.1900794
Mean score of good states: 493.563735, bad states: 532.701435
Std score of good states: 2262.894736, bad states: 1112.887651
Min score of good states: -223089.826022, bad states: -108806.809950
Max score of good states: 621.051757, bad states: 615.210353
Bad percentiles:
0 percentile: -108806.809950
10 percentile: 440.070313
20 percentile: 560.267168
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 615.210353
Good percentiles:
0 percentile: -223089.826022
10 percentile: 429.650651
20 percentile: 480.701117
30 percentile: 513.791461
40 percentile: 533.037292
50 percentile: 547.224952
60 percentile: 557.897706
70 percentile: 567.687419
80 percentile: 577.035790
90 percentile: 587.618213
100 percentile: 621.051757

======================================================

Generating 100 trajectories for policy iter0001000...
Calculating scores...
Dataset size: 92789 transitions (100 trajectories)
Average return: 9310.87323427
<type 'numpy.ndarray'>
Dataset size: 92789 transitions (100 trajectories)
Average return: 9310.87323427
<type 'numpy.ndarray'>
upper_bound_reward:  10172.8782987
Mean score of good states: 506.726880, bad states: 110.741841
Std score of good states: 460.430253, bad states: 41217.746708
Min score of good states: -26727.059736, bad states: -4121056.794921
Max score of good states: 624.902820, bad states: 616.673454
Bad percentiles:
0 percentile: -4121056.794921
10 percentile: 429.392991
20 percentile: 512.972805
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 616.673454
Good percentiles:
0 percentile: -26727.059736
10 percentile: 423.633586
20 percentile: 478.387614
30 percentile: 511.197198
40 percentile: 532.686862
50 percentile: 546.585158
60 percentile: 557.218304
70 percentile: 567.019617
80 percentile: 576.308325
90 percentile: 586.526551
100 percentile: 624.902820

======================================================

Generating 100 trajectories for policy iter0001020...
Calculating scores...
Dataset size: 93439 transitions (100 trajectories)
Average return: 9384.63238594
<type 'numpy.ndarray'>
Dataset size: 93439 transitions (100 trajectories)
Average return: 9384.63238594
<type 'numpy.ndarray'>
upper_bound_reward:  10186.794681
Mean score of good states: 513.046092, bad states: 313.183947
Std score of good states: 203.084408, bad states: 14256.800747
Min score of good states: -10732.962837, bad states: -1126802.479139
Max score of good states: 620.868524, bad states: 617.146204
Bad percentiles:
0 percentile: -1126802.479139
10 percentile: 429.791732
20 percentile: 506.098142
30 percentile: 558.450584
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 617.146204
Good percentiles:
0 percentile: -10732.962837
10 percentile: 416.020100
20 percentile: 470.969152
30 percentile: 506.926986
40 percentile: 528.153117
50 percentile: 543.450817
60 percentile: 555.707411
70 percentile: 565.500917
80 percentile: 575.184303
90 percentile: 586.045729
100 percentile: 620.868524

======================================================

Generating 100 trajectories for policy iter0001040...
Calculating scores...
Dataset size: 89980 transitions (100 trajectories)
Average return: 9023.49998409
<type 'numpy.ndarray'>
Dataset size: 89980 transitions (100 trajectories)
Average return: 9023.49998409
<type 'numpy.ndarray'>
upper_bound_reward:  10186.0243248
Mean score of gLoading policy parameters from /snapshots/iter0001060 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001060
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9114.89621872
Loading policy parameters from /snapshots/iter0001080 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001080
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8916.72997305
Loading policy parameters from /snapshots/iter0001100 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001100
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9163.18490158
ood states: 521.172809, bad states: 548.234968
Std score of good states: 141.387232, bad states: 430.293929
Min score of good states: -8713.744757, bad states: -25643.177113
Max score of good states: 621.317220, bad states: 608.176230
Bad percentiles:
0 percentile: -25643.177113
10 percentile: 458.213280
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 608.176230
Good percentiles:
0 percentile: -8713.744757
10 percentile: 424.121145
20 percentile: 481.731462
30 percentile: 514.288929
40 percentile: 534.002240
50 percentile: 548.242390
60 percentile: 559.164686
70 percentile: 568.846741
80 percentile: 577.871380
90 percentile: 587.667864
100 percentile: 621.317220

======================================================

Generating 100 trajectories for policy iter0001060...
Calculating scores...
Dataset size: 90676 transitions (100 trajectories)
Average return: 9114.89621872
<type 'numpy.ndarray'>
Dataset size: 90676 transitions (100 trajectories)
Average return: 9114.89621872
<type 'numpy.ndarray'>
upper_bound_reward:  10206.6404731
Mean score of good states: 513.655070, bad states: 354.071751
Std score of good states: 338.333978, bad states: 10893.350779
Min score of good states: -17280.325120, bad states: -845259.470858
Max score of good states: 622.260704, bad states: 585.248891
Bad percentiles:
0 percentile: -845259.470858
10 percentile: 444.256438
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 585.248891
Good percentiles:
0 percentile: -17280.325120
10 percentile: 420.197883
20 percentile: 476.660527
30 percentile: 510.501621
40 percentile: 531.827345
50 percentile: 547.306846
60 percentile: 558.617374
70 percentile: 568.664503
80 percentile: 578.380215
90 percentile: 588.707463
100 percentile: 622.260704

======================================================

Generating 100 trajectories for policy iter0001080...
Calculating scores...
Dataset size: 88779 transitions (100 trajectories)
Average return: 8916.72997305
<type 'numpy.ndarray'>
Dataset size: 88779 transitions (100 trajectories)
Average return: 8916.72997305
<type 'numpy.ndarray'>
upper_bound_reward:  10205.983004
Mean score of good states: 513.984361, bad states: -683.337924
Std score of good states: 401.062221, bad states: 119935.325463
Min score of good states: -30785.985411, bad states: -11991416.603651
Max score of good states: 623.250813, bad states: 607.355859
Bad percentiles:
0 percentile: -11991416.603651
10 percentile: 444.002145
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 607.355859
Good percentiles:
0 percentile: -30785.985411
10 percentile: 422.648777
20 percentile: 478.582098
30 percentile: 512.174454
40 percentile: 531.940603
50 percentile: 546.843830
60 percentile: 558.110309
70 percentile: 567.228890
80 percentile: 576.993341
90 percentile: 587.064689
100 percentile: 623.250813

======================================================

Generating 100 trajectories for policy iter0001100...
Calculating scores...
Dataset size: 91309 transitions (100 trajectories)
Average return: 9163.18490158
<type 'numpy.ndarray'>
Dataset size: 91309 transitions (100 trajectories)
Average return: 9163.18490158
<type 'numpy.ndarray'>
upper_bound_reward:  10180.888674
Mean score of good states: 514.942996, bad states: 539.043442
Std score of good states: 232.766097, bad states: 213.938756
Min score of good states: -15703.596398, bad states: -10121.241799
Max score of good states: 623.592679, bad states: 611.541725
Bad percentiles:
0 percentile: -10121.241799
10 percentile: 426.038001
20 percentile: 525.469259
30 percentile: 583.422610
40 percLoading policy parameters from /snapshots/iter0001120 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001120
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9239.95544587
Loading policy parameters from /snapshots/iter0001140 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001140
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8843.38241297
Loading policy parameters from /snapshots/iter0001160 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001160
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9493.51972732
entile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 611.541725
Good percentiles:
0 percentile: -15703.596398
10 percentile: 420.830757
20 percentile: 476.098839
30 percentile: 509.694306
40 percentile: 530.186674
50 percentile: 544.883926
60 percentile: 556.639082
70 percentile: 566.403758
80 percentile: 575.845822
90 percentile: 586.427191
100 percentile: 623.592679

======================================================

Generating 100 trajectories for policy iter0001120...
Calculating scores...
Dataset size: 91930 transitions (100 trajectories)
Average return: 9239.95544587
<type 'numpy.ndarray'>
Dataset size: 91930 transitions (100 trajectories)
Average return: 9239.95544587
<type 'numpy.ndarray'>
upper_bound_reward:  10163.2468087
Mean score of good states: 510.871979, bad states: -1152.371959
Std score of good states: 348.904409, bad states: 97300.498387
Min score of good states: -21373.919978, bad states: -8810588.566110
Max score of good states: 623.202636, bad states: 613.595230
Bad percentiles:
0 percentile: -8810588.566110
10 percentile: 434.067101
20 percentile: 543.420986
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 613.595230
Good percentiles:
0 percentile: -21373.919978
10 percentile: 425.365144
20 percentile: 476.081179
30 percentile: 509.575952
40 percentile: 529.719720
50 percentile: 544.684929
60 percentile: 555.684519
70 percentile: 565.533222
80 percentile: 574.874972
90 percentile: 586.103865
100 percentile: 623.202636

======================================================

Generating 100 trajectories for policy iter0001140...
Calculating scores...
Dataset size: 88148 transitions (100 trajectories)
Average return: 8843.38241297
<type 'numpy.ndarray'>
Dataset size: 88148 transitions (100 trajectories)
Average return: 8843.38241297
<type 'numpy.ndarray'>
upper_bound_reward:  10191.166379
Mean score of good states: 513.799637, bad states: 545.602516
Std score of good states: 194.304158, bad states: 263.056528
Min score of good states: -10420.950338, bad states: -14060.248621
Max score of good states: 623.872228, bad states: 623.411464
Bad percentiles:
0 percentile: -14060.248621
10 percentile: 439.282537
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 623.411464
Good percentiles:
0 percentile: -10420.950338
10 percentile: 406.828315
20 percentile: 473.598063
30 percentile: 510.282467
40 percentile: 530.745797
50 percentile: 545.643869
60 percentile: 556.813044
70 percentile: 566.240356
80 percentile: 575.825816
90 percentile: 586.438320
100 percentile: 623.872228

======================================================

Generating 100 trajectories for policy iter0001160...
Calculating scores...
Dataset size: 94456 transitions (100 trajectories)
Average return: 9493.51972732
<type 'numpy.ndarray'>
Dataset size: 94456 transitions (100 trajectories)
Average return: 9493.51972732
<type 'numpy.ndarray'>
upper_bound_reward:  10169.546238
Mean score of good states: 515.604239, bad states: 524.689746
Std score of good states: 321.368645, bad states: 373.014880
Min score of good states: -23386.356261, bad states: -15053.555109
Max score of good states: 625.902887, bad states: 616.662266
Bad percentiles:
0 percentile: -15053.555109
10 percentile: 430.656188
20 percentile: 494.918450
30 percentile: 540.620918
40 percentile: 573.137682
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 616.662266
Good percentiles:
0 percentile: -23386.356261
10 percentile: 428.690551
20 percentile: 480.825892
30 percentile: 511.761141
40 percentile: 530.855544
50 percentile: 544.606221
60 percentile: 556Loading policy parameters from /snapshots/iter0001180 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001180
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9273.27514597
Loading policy parameters from /snapshots/iter0001200 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001200
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9222.80863643
Loading policy parameters from /snapshots/iter0001220 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001220
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8722.29057311
Loading policy parameters from /snapshots/iter0001240 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001240
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9451.48222737
.205522
70 percentile: 566.112474
80 percentile: 575.356961
90 percentile: 586.222416
100 percentile: 625.902887

======================================================

Generating 100 trajectories for policy iter0001180...
Calculating scores...
Dataset size: 92272 transitions (100 trajectories)
Average return: 9273.27514597
<type 'numpy.ndarray'>
Dataset size: 92272 transitions (100 trajectories)
Average return: 9273.27514597
<type 'numpy.ndarray'>
upper_bound_reward:  10188.0326582
Mean score of good states: 1.124491, bad states: 526.627553
Std score of good states: 36810.817463, bad states: 456.259107
Min score of good states: -3017171.079467, bad states: -23161.681034
Max score of good states: 622.463103, bad states: 614.516584
Bad percentiles:
0 percentile: -23161.681034
10 percentile: 429.374659
20 percentile: 511.484671
30 percentile: 562.715505
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 614.516584
Good percentiles:
0 percentile: -3017171.079467
10 percentile: 415.861114
20 percentile: 475.019160
30 percentile: 509.560969
40 percentile: 530.568248
50 percentile: 545.233358
60 percentile: 556.566859
70 percentile: 566.611418
80 percentile: 575.595532
90 percentile: 586.109558
100 percentile: 622.463103

======================================================

Generating 100 trajectories for policy iter0001200...
Calculating scores...
Dataset size: 91901 transitions (100 trajectories)
Average return: 9222.80863643
<type 'numpy.ndarray'>
Dataset size: 91901 transitions (100 trajectories)
Average return: 9222.80863643
<type 'numpy.ndarray'>
upper_bound_reward:  10205.3701162
Mean score of good states: 514.214908, bad states: 233.723980
Std score of good states: 287.013844, bad states: 17400.053397
Min score of good states: -17886.945793, bad states: -1297048.923362
Max score of good states: 625.329978, bad states: 617.684405
Bad percentiles:
0 percentile: -1297048.923362
10 percentile: 430.732657
20 percentile: 531.949192
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 617.684405
Good percentiles:
0 percentile: -17886.945793
10 percentile: 426.884371
20 percentile: 476.706365
30 percentile: 509.704078
40 percentile: 528.817528
50 percentile: 543.784014
60 percentile: 555.658274
70 percentile: 566.152701
80 percentile: 575.838207
90 percentile: 586.426561
100 percentile: 625.329978

======================================================

Generating 100 trajectories for policy iter0001220...
Calculating scores...
Dataset size: 86807 transitions (100 trajectories)
Average return: 8722.29057311
<type 'numpy.ndarray'>
Dataset size: 86807 transitions (100 trajectories)
Average return: 8722.29057311
<type 'numpy.ndarray'>
upper_bound_reward:  10241.5242416
Mean score of good states: 519.415245, bad states: -3563.502140
Std score of good states: 186.060093, bad states: 365642.495715
Min score of good states: -10670.084187, bad states: -36476057.500180
Max score of good states: 623.327456, bad states: 613.145518
Bad percentiles:
0 percentile: -36476057.500180
10 percentile: 437.339508
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 613.145518
Good percentiles:
0 percentile: -10670.084187
10 percentile: 429.163986
20 percentile: 479.833365
30 percentile: 513.079143
40 percentile: 532.501549
50 percentile: 546.502796
60 percentile: 557.408375
70 percentile: 567.403054
80 percentile: 576.555344
90 percentile: 587.315085
100 percentile: 623.327456

======================================================

Generating 100 trajectories for policy iter0001240...
Calculating scores...
Dataset size: 93691 transitions (100 trajectories)
Average return: 9451.48222737
<type 'numpy.ndarray'>
DatasLoading policy parameters from /snapshots/iter0001260 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001260
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9356.02000727
Loading policy parameters from /snapshots/iter0001280 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001280
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8968.26582448
Loading policy parameters from /snapshots/iter0001300 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001300
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8763.11429327
et size: 93691 transitions (100 trajectories)
Average return: 9451.48222737
<type 'numpy.ndarray'>
upper_bound_reward:  10214.5910218
Mean score of good states: 508.264346, bad states: 395.165822
Std score of good states: 470.168339, bad states: 7755.281325
Min score of good states: -33268.814244, bad states: -697972.420263
Max score of good states: 620.159910, bad states: 616.599187
Bad percentiles:
0 percentile: -697972.420263
10 percentile: 408.651177
20 percentile: 485.173634
30 percentile: 557.173012
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 616.599187
Good percentiles:
0 percentile: -33268.814244
10 percentile: 425.565374
20 percentile: 477.424575
30 percentile: 510.911942
40 percentile: 531.130279
50 percentile: 545.139582
60 percentile: 556.116360
70 percentile: 565.984124
80 percentile: 576.172098
90 percentile: 586.275220
100 percentile: 620.159910

======================================================

Generating 100 trajectories for policy iter0001260...
Calculating scores...
Dataset size: 92958 transitions (100 trajectories)
Average return: 9356.02000727
<type 'numpy.ndarray'>
Dataset size: 92958 transitions (100 trajectories)
Average return: 9356.02000727
<type 'numpy.ndarray'>
upper_bound_reward:  10193.3030122
Mean score of good states: 520.977088, bad states: 524.358410
Std score of good states: 232.000458, bad states: 530.551632
Min score of good states: -13569.796199, bad states: -29953.234044
Max score of good states: 620.103288, bad states: 617.476500
Bad percentiles:
0 percentile: -29953.234044
10 percentile: 432.001667
20 percentile: 507.254150
30 percentile: 564.430551
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 617.476500
Good percentiles:
0 percentile: -13569.796199
10 percentile: 430.048454
20 percentile: 483.291247
30 percentile: 515.379746
40 percentile: 534.928235
50 percentile: 549.246428
60 percentile: 559.843076
70 percentile: 568.884992
80 percentile: 578.311941
90 percentile: 588.120697
100 percentile: 620.103288

======================================================

Generating 100 trajectories for policy iter0001280...
Calculating scores...
Dataset size: 89113 transitions (100 trajectories)
Average return: 8968.26582448
<type 'numpy.ndarray'>
Dataset size: 89113 transitions (100 trajectories)
Average return: 8968.26582448
<type 'numpy.ndarray'>
upper_bound_reward:  10223.0709718
Mean score of good states: 516.060028, bad states: -971.373636
Std score of good states: 399.048917, bad states: 115909.915024
Min score of good states: -36034.048964, bad states: -11338200.315102
Max score of good states: 623.601340, bad states: 599.013292
Bad percentiles:
0 percentile: -11338200.315102
10 percentile: 428.882259
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 599.013292
Good percentiles:
0 percentile: -36034.048964
10 percentile: 423.560338
20 percentile: 479.219331
30 percentile: 511.785163
40 percentile: 531.691036
50 percentile: 546.524284
60 percentile: 558.170667
70 percentile: 568.154101
80 percentile: 577.489431
90 percentile: 587.076487
100 percentile: 623.601340

======================================================

Generating 100 trajectories for policy iter0001300...
Calculating scores...
Dataset size: 87340 transitions (100 trajectories)
Average return: 8763.11429327
<type 'numpy.ndarray'>
Dataset size: 87340 transitions (100 trajectories)
Average return: 8763.11429327
<type 'numpy.ndarray'>
upper_bound_reward:  10181.6075398
Mean score of good states: 513.172048, bad states: 522.841112
Std score of good states: 288.723304, bad states: 1435.521357
Min score of good states: -14056.126614, bad states: -134627.644087
Max score of good states: 622.074334,Loading policy parameters from /snapshots/iter0001320 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001320
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8737.3094916
Loading policy parameters from /snapshots/iter0001340 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001340
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9064.80167433
Loading policy parameters from /snapshots/iter0001360 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001360
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8514.59866035
 bad states: 600.354238
Bad percentiles:
0 percentile: -134627.644087
10 percentile: 437.836064
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 600.354238
Good percentiles:
0 percentile: -14056.126614
10 percentile: 424.276042
20 percentile: 478.362956
30 percentile: 511.076814
40 percentile: 530.846733
50 percentile: 544.770377
60 percentile: 556.479265
70 percentile: 565.752978
80 percentile: 575.276306
90 percentile: 585.457221
100 percentile: 622.074334

======================================================

Generating 100 trajectories for policy iter0001320...
Calculating scores...
Dataset size: 87284 transitions (100 trajectories)
Average return: 8737.3094916
<type 'numpy.ndarray'>
Dataset size: 87284 transitions (100 trajectories)
Average return: 8737.3094916
<type 'numpy.ndarray'>
upper_bound_reward:  10198.7175376
Mean score of good states: 513.872214, bad states: 511.245091
Std score of good states: 286.427315, bad states: 3103.099452
Min score of good states: -13953.244129, bad states: -306489.004094
Max score of good states: 622.043584, bad states: 596.688210
Bad percentiles:
0 percentile: -306489.004094
10 percentile: 435.450941
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 596.688210
Good percentiles:
0 percentile: -13953.244129
10 percentile: 422.618811
20 percentile: 478.558191
30 percentile: 510.730913
40 percentile: 530.086475
50 percentile: 544.694922
60 percentile: 556.227162
70 percentile: 566.054123
80 percentile: 575.188892
90 percentile: 585.938336
100 percentile: 622.043584

======================================================

Generating 100 trajectories for policy iter0001340...
Calculating scores...
Dataset size: 90314 transitions (100 trajectories)
Average return: 9064.80167433
<type 'numpy.ndarray'>
Dataset size: 90314 transitions (100 trajectories)
Average return: 9064.80167433
<type 'numpy.ndarray'>
upper_bound_reward:  10193.8772091
Mean score of good states: 502.872220, bad states: 528.481828
Std score of good states: 438.043907, bad states: 459.141219
Min score of good states: -15773.709213, bad states: -21802.252938
Max score of good states: 618.559372, bad states: 619.256642
Bad percentiles:
0 percentile: -21802.252938
10 percentile: 411.878566
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 619.256642
Good percentiles:
0 percentile: -15773.709213
10 percentile: 418.978431
20 percentile: 473.080190
30 percentile: 507.249931
40 percentile: 527.836643
50 percentile: 542.737906
60 percentile: 554.846363
70 percentile: 564.986289
80 percentile: 574.984920
90 percentile: 585.847776
100 percentile: 618.559372

======================================================

Generating 100 trajectories for policy iter0001360...
Calculating scores...
Dataset size: 85181 transitions (100 trajectories)
Average return: 8514.59866035
<type 'numpy.ndarray'>
Dataset size: 85181 transitions (100 trajectories)
Average return: 8514.59866035
<type 'numpy.ndarray'>
upper_bound_reward:  10173.1597547
Mean score of good states: 497.695246, bad states: 532.639633
Std score of good states: 612.605912, bad states: 448.272970
Min score of good states: -42397.768264, bad states: -28299.839338
Max score of good states: 620.076540, bad states: 583.422610
Bad percentiles:
0 percentile: -28299.839338
10 percentile: 426.623066
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -42397.76826Loading policy parameters from /snapshots/iter0001380 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001380
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8854.65967083
Loading policy parameters from /snapshots/iter0001400 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001400
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8516.35301139
Loading policy parameters from /snapshots/iter0001420 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001420
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9046.78788332
4
10 percentile: 416.828405
20 percentile: 469.357322
30 percentile: 505.297983
40 percentile: 527.187045
50 percentile: 541.499994
60 percentile: 553.519269
70 percentile: 563.181263
80 percentile: 572.996434
90 percentile: 584.142586
100 percentile: 620.076540

======================================================

Generating 100 trajectories for policy iter0001380...
Calculating scores...
Dataset size: 88260 transitions (100 trajectories)
Average return: 8854.65967083
<type 'numpy.ndarray'>
Dataset size: 88260 transitions (100 trajectories)
Average return: 8854.65967083
<type 'numpy.ndarray'>
upper_bound_reward:  10176.4912199
Mean score of good states: 516.653768, bad states: 486.048123
Std score of good states: 236.825032, bad states: 4883.442557
Min score of good states: -10041.397089, bad states: -483833.908578
Max score of good states: 621.013313, bad states: 608.907543
Bad percentiles:
0 percentile: -483833.908578
10 percentile: 431.251993
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 608.907543
Good percentiles:
0 percentile: -10041.397089
10 percentile: 424.034251
20 percentile: 480.849029
30 percentile: 513.392701
40 percentile: 533.672048
50 percentile: 547.593531
60 percentile: 558.725242
70 percentile: 567.532132
80 percentile: 576.457517
90 percentile: 586.689807
100 percentile: 621.013313

======================================================

Generating 100 trajectories for policy iter0001400...
Calculating scores...
Dataset size: 85259 transitions (100 trajectories)
Average return: 8516.35301139
<type 'numpy.ndarray'>
Dataset size: 85259 transitions (100 trajectories)
Average return: 8516.35301139
<type 'numpy.ndarray'>
upper_bound_reward:  10184.598033
Mean score of good states: 480.902485, bad states: 223.962006
Std score of good states: 3503.852563, bad states: 30345.653054
Min score of good states: -348117.429922, bad states: -3033822.264962
Max score of good states: 623.250480, bad states: 583.422610
Bad percentiles:
0 percentile: -3033822.264962
10 percentile: 412.750677
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -348117.429922
10 percentile: 426.761535
20 percentile: 480.428703
30 percentile: 510.918683
40 percentile: 531.194175
50 percentile: 546.700049
60 percentile: 558.004597
70 percentile: 568.026499
80 percentile: 577.185005
90 percentile: 587.147726
100 percentile: 623.250480

======================================================

Generating 100 trajectories for policy iter0001420...
Calculating scores...
Dataset size: 89964 transitions (100 trajectories)
Average return: 9046.78788332
<type 'numpy.ndarray'>
Dataset size: 89964 transitions (100 trajectories)
Average return: 9046.78788332
<type 'numpy.ndarray'>
upper_bound_reward:  10191.380509
Mean score of good states: 520.207058, bad states: 532.528198
Std score of good states: 158.193022, bad states: 519.972923
Min score of good states: -9552.858233, bad states: -27967.162178
Max score of good states: 622.239849, bad states: 598.804990
Bad percentiles:
0 percentile: -27967.162178
10 percentile: 431.479038
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 598.804990
Good percentiles:
0 percentile: -9552.858233
10 percentile: 423.926946
20 percentile: 481.571615
30 percentile: 514.462542
40 percentile: 534.271209
50 percentile: 547.875714
60 percentile: 558.247652
70 percentile: 567.652503
80 percentile: 576.722316
90 percentile: 586.532866
100 percentile: 622.239849

======================================================

Generating 100 trajectories for policy iter00014Loading policy parameters from /snapshots/iter0001440 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001440
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8749.11337685
Loading policy parameters from /snapshots/iter0001460 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001460
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9400.57351466
Loading policy parameters from /snapshots/iter0001480 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001480
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 9073.16605662
Loading policy parameters from /snapshots/iter0001500 in ../training_logs/additiveStatePrior/no_step_trashing_with_bad_state_logging_policy.h5
{u'CLASS': 'GROUP',
 u'TITLE': '',
 u'VERSION': '1.0',
 u'hash': 'ac3e9507b7345204671d261b95d9ef4a54bb6b34'}
Loading environment Humanoid-v1
Gym version: 0.9.1
[95mMDP observation space, action space sizes: 376, 17
[0m
[95mMax traj len is 1000[0m
[95mLoading feedforward net specification[0m
[
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  },
  {
    "type": "fc",
    "n": 100
  },
  {
    "type": "nonlin",
    "func": "tanh"
  }
]
[95mAffine(in=376, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=100)[0m
[95mNonlinearity(func=tanh)[0m
[95mAffine(in=100, out=17)[0m
Reading GaussianPolicy/logstdevs_1_Da
Reading GaussianPolicy/obsnorm/Standardizer/count
Reading GaussianPolicy/obsnorm/Standardizer/mean_1_D
Reading GaussianPolicy/obsnorm/Standardizer/meansq_1_D
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_0/AffineLayer/b
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/W
Reading GaussianPolicy/hidden/FeedforwardNet/layer_2/AffineLayer/b
Reading GaussianPolicy/out/AffineLayer/W
Reading GaussianPolicy/out/AffineLayer/b
Saving traj samples to file: ../trajectories/HyperparamAnalysis/iter0001500
[95mSampling 100 trajectories of maximum length 1000[0m
Average return: 8483.38119244
40...
Calculating scores...
Dataset size: 87193 transitions (100 trajectories)
Average return: 8749.11337685
<type 'numpy.ndarray'>
Dataset size: 87193 transitions (100 trajectories)
Average return: 8749.11337685
<type 'numpy.ndarray'>
upper_bound_reward:  10195.6180334
Mean score of good states: 517.524266, bad states: 542.288492
Std score of good states: 229.856943, bad states: 406.223506
Min score of good states: -15570.663552, bad states: -17789.606561
Max score of good states: 624.620880, bad states: 595.978982
Bad percentiles:
0 percentile: -17789.606561
10 percentile: 434.284197
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 595.978982
Good percentiles:
0 percentile: -15570.663552
10 percentile: 425.296125
20 percentile: 478.997429
30 percentile: 511.616238
40 percentile: 532.750835
50 percentile: 546.436672
60 percentile: 557.675027
70 percentile: 567.232846
80 percentile: 576.800557
90 percentile: 586.914519
100 percentile: 624.620880

======================================================

Generating 100 trajectories for policy iter0001460...
Calculating scores...
Dataset size: 93329 transitions (100 trajectories)
Average return: 9400.57351466
<type 'numpy.ndarray'>
Dataset size: 93329 transitions (100 trajectories)
Average return: 9400.57351466
<type 'numpy.ndarray'>
upper_bound_reward:  10202.3898496
Mean score of good states: 507.418215, bad states: 505.012127
Std score of good states: 514.637672, bad states: 890.349103
Min score of good states: -35268.215390, bad states: -68695.693402
Max score of good states: 619.907065, bad states: 624.787294
Bad percentiles:
0 percentile: -68695.693402
10 percentile: 419.107677
20 percentile: 496.942613
30 percentile: 555.141629
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 624.787294
Good percentiles:
0 percentile: -35268.215390
10 percentile: 418.113555
20 percentile: 479.095530
30 percentile: 513.506926
40 percentile: 534.268693
50 percentile: 547.609401
60 percentile: 558.203852
70 percentile: 567.818764
80 percentile: 576.546156
90 percentile: 586.683929
100 percentile: 619.907065

======================================================

Generating 100 trajectories for policy iter0001480...
Calculating scores...
Dataset size: 90372 transitions (100 trajectories)
Average return: 9073.16605662
<type 'numpy.ndarray'>
Dataset size: 90372 transitions (100 trajectories)
Average return: 9073.16605662
<type 'numpy.ndarray'>
upper_bound_reward:  10186.8774504
Mean score of good states: 510.087599, bad states: -8127.489747
Std score of good states: 878.578448, bad states: 867452.048895
Min score of good states: -85074.226675, bad states: -86748990.998218
Max score of good states: 621.046013, bad states: 583.422610
Bad percentiles:
0 percentile: -86748990.998218
10 percentile: 438.646644
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -85074.226675
10 percentile: 420.818907
20 percentile: 480.533498
30 percentile: 514.046647
40 percentile: 533.390900
50 percentile: 548.241902
60 percentile: 558.509962
70 percentile: 567.634061
80 percentile: 576.517704
90 percentile: 586.451234
100 percentile: 621.046013

======================================================

Generating 100 trajectories for policy iter0001500...
Calculating scores...
Dataset size: 84896 transitions (100 trajectories)
Average return: 8483.38119244
<type 'numpy.ndarray'>
Dataset size: 84896 transitions (100 trajectories)
Average return: 8483.38119244
<type 'numpy.ndarray'>
upper_bound_reward:  10160.80885
Mean score of good states: 514.794736, bad states: 542.040671
Std score of good states: 275.523892, bad states: 465.996978
Min score of good states: -16171.333898, bad states: -12588.588368
Max score of good states: 622.041825, bad states: 583.422610
Bad percentiles:
0 percentile: -12588.588368
10 percentile: 466.520735
20 percentile: 583.422610
30 percentile: 583.422610
40 percentile: 583.422610
50 percentile: 583.422610
60 percentile: 583.422610
70 percentile: 583.422610
80 percentile: 583.422610
90 percentile: 583.422610
100 percentile: 583.422610
Good percentiles:
0 percentile: -16171.333898
10 percentile: 423.224387
20 percentile: 479.585297
30 percentile: 512.017480
40 percentile: 532.511803
50 percentile: 546.681622
60 percentile: 557.349727
70 percentile: 566.984602
80 percentile: 576.107712
90 percentile: 586.618852
100 percentile: 622.041825

======================================================

